name: "Complete CI/CD Agent Review Pipeline"

on:
  schedule:
    # Run every 12 hours (at 00:00 and 12:00 UTC)
    - cron: '0 0,12 * * *'
  push:
    branches:
      - main
      - master
  pull_request:
    types: [opened, synchronize, reopened]
  workflow_dispatch:
    inputs:
      skip_tests:
        description: 'Skip test execution'
        required: false
        default: 'false'
        type: boolean
      skip_docs:
        description: 'Skip documentation review'
        required: false
        default: 'false'
        type: boolean

permissions:
  contents: write
  pull-requests: write
  issues: write
  checks: write
  actions: read

jobs:
  # Step 1: Code Cleanliness Review
  code-cleanliness:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@main
        with:
          fetch-depth: 0

      - name: Run Code Cleanliness Analysis
        run: |
          echo "ðŸ” Running code cleanliness analysis..."
          
          # Create results directory
          mkdir -p /tmp/review-results
          
          echo "## Code Cleanliness Analysis" > /tmp/review-results/cleanliness.md
          echo "" >> /tmp/review-results/cleanliness.md
          
          # Find large files
          echo "### Large Files (>500 lines):" >> /tmp/review-results/cleanliness.md
          find . -type f \( -name "*.py" -o -name "*.js" -o -name "*.ts" -o -name "*.java" -o -name "*.go" -o -name "*.cs" \) \
            ! -path "*/node_modules/*" ! -path "*/dist/*" ! -path "*/build/*" ! -path "*/.venv/*" \
            -exec sh -c 'lines=$(wc -l < "$1"); if [ "$lines" -gt 500 ]; then echo "$lines lines: $1"; fi' _ {} \; \
            | sort -rn >> /tmp/review-results/cleanliness.md || echo "No large files found" >> /tmp/review-results/cleanliness.md
          
          echo "âœ… Code cleanliness analysis complete"

      - name: Upload Cleanliness Report
        uses: actions/upload-artifact@main
        with:
          name: cleanliness-report
          path: /tmp/review-results/cleanliness.md
          retention-days: 30

  # Step 2: Test Review and Execution
  test-review:
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_tests != 'true'
    strategy:
      fail-fast: false
      matrix:
        test-type: [unit, integration, e2e]
    steps:
      - name: Checkout code
        uses: actions/checkout@main

      - name: Setup Test Environment
        run: |
          echo "ðŸ§ª Setting up test environment for ${{ matrix.test-type }} tests..."
          mkdir -p /tmp/review-results

      - name: Setup Node.js
        uses: actions/setup-node@main
        with:
          node-version: '20'
        continue-on-error: true

      - name: Setup Python
        uses: actions/setup-python@main
        with:
          python-version: '3.11'
        continue-on-error: true

      - name: Install Playwright for E2E
        if: matrix.test-type == 'e2e'
        run: |
          if [ -f "package.json" ]; then
            npm install
            npm install -D @playwright/test playwright
            npx playwright install --with-deps chromium firefox webkit
          fi
          pip install pytest playwright pytest-playwright
          python -m playwright install --with-deps chromium firefox webkit
        continue-on-error: true

      - name: Run Tests - ${{ matrix.test-type }}
        id: run_tests
        run: |
          echo "Running ${{ matrix.test-type }} tests..."
          TEST_STATUS="not-found"
          TEST_OUTPUT=""
          
          case "${{ matrix.test-type }}" in
            unit)
              if [ -f "package.json" ] && grep -q '"test"' package.json; then
                echo "ðŸ“ Running Node.js unit tests..."
                if npm test -- --testPathPattern="unit" 2>&1 | tee /tmp/test-output.txt || npm test 2>&1 | tee /tmp/test-output.txt; then
                  TEST_STATUS="passed"
                  TEST_OUTPUT="Unit tests: âœ… Passed"
                else
                  TEST_STATUS="failed"
                  TEST_OUTPUT="Unit tests: âŒ Failed (see artifacts for details)"
                fi
              fi
              if command -v pytest &> /dev/null && [ -d "tests/unit/" ]; then
                echo "ðŸ Running Python unit tests..."
                if pytest tests/unit/ 2>&1 | tee -a /tmp/test-output.txt; then
                  TEST_STATUS="passed"
                  TEST_OUTPUT="${TEST_OUTPUT}\nPython unit tests: âœ… Passed"
                else
                  TEST_STATUS="failed"
                  TEST_OUTPUT="${TEST_OUTPUT}\nPython unit tests: âŒ Failed"
                fi
              fi
              ;;
            integration)
              if command -v pytest &> /dev/null && [ -d "tests/integration/" ]; then
                echo "ðŸ Running Python integration tests..."
                if pytest tests/integration/ 2>&1 | tee /tmp/test-output.txt; then
                  TEST_STATUS="passed"
                  TEST_OUTPUT="Python integration tests: âœ… Passed"
                else
                  TEST_STATUS="failed"
                  TEST_OUTPUT="Python integration tests: âŒ Failed"
                fi
              fi
              if [ -f "package.json" ]; then
                echo "ðŸ“ Running JS integration tests..."
                if npm test -- --testPathPattern="integration" 2>&1 | tee -a /tmp/test-output.txt; then
                  TEST_STATUS="passed"
                  TEST_OUTPUT="${TEST_OUTPUT}\nJS integration tests: âœ… Passed"
                else
                  [ "$TEST_STATUS" = "not-found" ] && TEST_STATUS="failed"
                  TEST_OUTPUT="${TEST_OUTPUT}\nJS integration tests: âŒ Failed"
                fi
              fi
              ;;
            e2e)
              if command -v npx &> /dev/null; then
                echo "ðŸŽ­ Running Playwright tests..."
                if npx playwright test 2>&1 | tee /tmp/test-output.txt; then
                  TEST_STATUS="passed"
                  TEST_OUTPUT="Playwright tests: âœ… Passed"
                else
                  TEST_STATUS="failed"
                  TEST_OUTPUT="Playwright tests: âŒ Failed"
                fi
              fi
              if command -v pytest &> /dev/null && [ -d "tests/e2e/" ]; then
                echo "ðŸ Running Python E2E tests..."
                if pytest tests/e2e/ 2>&1 | tee -a /tmp/test-output.txt || pytest --browser chromium 2>&1 | tee -a /tmp/test-output.txt; then
                  TEST_STATUS="passed"
                  TEST_OUTPUT="${TEST_OUTPUT}\nPython E2E tests: âœ… Passed"
                else
                  [ "$TEST_STATUS" = "not-found" ] && TEST_STATUS="failed"
                  TEST_OUTPUT="${TEST_OUTPUT}\nPython E2E tests: âŒ Failed"
                fi
              fi
              ;;
          esac
          
          echo "TEST_STATUS=$TEST_STATUS" >> $GITHUB_OUTPUT
          echo "TEST_OUTPUT<<EOF" >> $GITHUB_OUTPUT
          echo -e "$TEST_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Create Test Summary
        run: |
          mkdir -p /tmp/review-results
          echo "## Test Results - ${{ matrix.test-type }}" > /tmp/review-results/test-summary-${{ matrix.test-type }}.md
          echo "" >> /tmp/review-results/test-summary-${{ matrix.test-type }}.md
          echo "**Status:** ${{ steps.run_tests.outputs.TEST_STATUS }}" >> /tmp/review-results/test-summary-${{ matrix.test-type }}.md
          echo "" >> /tmp/review-results/test-summary-${{ matrix.test-type }}.md
          echo "${{ steps.run_tests.outputs.TEST_OUTPUT }}" >> /tmp/review-results/test-summary-${{ matrix.test-type }}.md
        continue-on-error: true

      - name: Upload Test Results
        uses: actions/upload-artifact@main
        if: always()
        with:
          name: test-results-${{ matrix.test-type }}
          path: |
            test-results/
            playwright-report/
            .pytest_cache/
            coverage/
            /tmp/test-output.txt
            /tmp/review-results/
          retention-days: 30
        continue-on-error: true

  # Step 3: Documentation Review
  documentation-review:
    runs-on: ubuntu-latest
    if: github.event.inputs.skip_docs != 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@main

      - name: Analyze Documentation
        run: |
          echo "ðŸ“š Analyzing documentation..."
          
          mkdir -p /tmp/review-results
          
          echo "## Documentation Analysis" > /tmp/review-results/documentation.md
          echo "" >> /tmp/review-results/documentation.md
          
          # Check for essential files
          echo "### Essential Documentation Files:" >> /tmp/review-results/documentation.md
          for doc in README.md CONTRIBUTING.md LICENSE.md CHANGELOG.md CODE_OF_CONDUCT.md SECURITY.md; do
            if [ -f "$doc" ]; then
              word_count=$(wc -w < "$doc" 2>/dev/null || echo 0)
              echo "âœ… $doc ($word_count words)" >> /tmp/review-results/documentation.md
            else
              echo "âŒ $doc (missing)" >> /tmp/review-results/documentation.md
            fi
          done
          
          # Check README quality
          if [ -f "README.md" ]; then
            echo "" >> /tmp/review-results/documentation.md
            echo "### README.md Content Check:" >> /tmp/review-results/documentation.md
            for section in "Installation" "Usage" "Features" "Contributing" "License" "Documentation" "Examples" "API"; do
              if grep -qi "$section" README.md; then
                echo "âœ… Contains '$section' section" >> /tmp/review-results/documentation.md
              else
                echo "âš ï¸ Missing '$section' section" >> /tmp/review-results/documentation.md
              fi
            done
          fi
          
          echo "âœ… Documentation analysis complete"

      - name: Upload Documentation Report
        uses: actions/upload-artifact@main
        with:
          name: documentation-report
          path: /tmp/review-results/documentation.md
          retention-days: 30

  # Step 4: Build and Functionality Check
  build-check:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@main

      - name: Setup Build Environment
        run: |
          echo "ðŸ—ï¸ Setting up build environment..."

      - name: Setup Node.js
        uses: actions/setup-node@main
        with:
          node-version: '20'
        continue-on-error: true

      - name: Setup Python
        uses: actions/setup-python@main
        with:
          python-version: '3.11'
        continue-on-error: true

      - name: Setup Go
        uses: actions/setup-go@main
        with:
          go-version: 'stable'
        continue-on-error: true

      - name: Build Project
        id: build
        run: |
          BUILD_STATUS="not-attempted"
          BUILD_DETAILS=""
          
          # Node.js
          if [ -f "package.json" ]; then
            echo "ðŸ“¦ Found Node.js project (package.json)"
            npm install || { echo "âš ï¸ npm install failed"; BUILD_STATUS="install-failed"; }
            
            if [ "$BUILD_STATUS" != "install-failed" ]; then
              if grep -q '"build"' package.json; then
                echo "ðŸ”¨ Running npm build..."
                if npm run build; then
                  BUILD_STATUS="success"
                  BUILD_DETAILS="${BUILD_DETAILS}Node.js build: âœ… Success\n"
                else
                  BUILD_STATUS="build-failed"
                  BUILD_DETAILS="${BUILD_DETAILS}Node.js build: âŒ Failed\n"
                fi
              else
                BUILD_STATUS="no-build-script"
                BUILD_DETAILS="${BUILD_DETAILS}Node.js: âš ï¸ No build script found in package.json\n"
              fi
            fi
          fi
          
          # Python
          if [ -f "requirements.txt" ]; then
            echo "ðŸ Found Python project (requirements.txt)"
            if pip install -r requirements.txt; then
              BUILD_STATUS="success"
              BUILD_DETAILS="${BUILD_DETAILS}Python dependencies: âœ… Installed\n"
            else
              [ "$BUILD_STATUS" = "not-attempted" ] && BUILD_STATUS="build-failed"
              BUILD_DETAILS="${BUILD_DETAILS}Python dependencies: âŒ Failed\n"
            fi
          fi
          
          # Go
          if [ -f "go.mod" ]; then
            echo "ðŸ¹ Found Go project (go.mod)"
            if go build ./...; then
              BUILD_STATUS="success"
              BUILD_DETAILS="${BUILD_DETAILS}Go build: âœ… Success\n"
            else
              [ "$BUILD_STATUS" = "not-attempted" ] && BUILD_STATUS="build-failed"
              BUILD_DETAILS="${BUILD_DETAILS}Go build: âŒ Failed\n"
            fi
          fi
          
          echo "BUILD_SUCCESS=$BUILD_STATUS" >> $GITHUB_OUTPUT
          echo "BUILD_DETAILS<<EOF" >> $GITHUB_OUTPUT
          echo -e "$BUILD_DETAILS" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
        continue-on-error: true

      - name: Upload Build Status
        run: |
          mkdir -p /tmp/review-results
          echo "## Build Status" > /tmp/review-results/build.md
          echo "" >> /tmp/review-results/build.md
          echo "**Overall Status:** ${{ steps.build.outputs.BUILD_SUCCESS }}" >> /tmp/review-results/build.md
          echo "" >> /tmp/review-results/build.md
          echo "### Build Details:" >> /tmp/review-results/build.md
          echo "" >> /tmp/review-results/build.md
          echo "${{ steps.build.outputs.BUILD_DETAILS }}" >> /tmp/review-results/build.md

      - name: Upload Build Report
        uses: actions/upload-artifact@main
        with:
          name: build-report
          path: /tmp/review-results/build.md
          retention-days: 30

  # Step 5: Consolidate Results and Create Report
  consolidate-results:
    runs-on: ubuntu-latest
    needs: [code-cleanliness, test-review, documentation-review, build-check]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@main

      - name: Download All Reports
        uses: actions/download-artifact@main
        with:
          path: /tmp/all-reports
        continue-on-error: true

      - name: Consolidate Reports
        run: |
          echo "ðŸ“Š Consolidating all reports..."
          
          mkdir -p /tmp/final-report
          
          cat > /tmp/final-report/complete-review.md << 'EOF'
          # Complete CI/CD Agent Review Report
          
          **Review Date:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Repository:** ${{ github.repository }}
          **Branch:** ${{ github.ref_name }}
          **Trigger:** ${{ github.event_name }}
          
          ## Executive Summary
          
          This comprehensive review covers:
          - âœ… Code cleanliness and file size analysis
          - âœ… Test coverage and Playwright integration
          - âœ… Documentation completeness and quality
          - âœ… Build functionality verification
          
          EOF
          
          # Append individual reports
          if [ -d "/tmp/all-reports" ]; then
            echo "" >> /tmp/final-report/complete-review.md
            echo "## Detailed Findings" >> /tmp/final-report/complete-review.md
            
            for report in /tmp/all-reports/*/*.md; do
              if [ -f "$report" ]; then
                echo "" >> /tmp/final-report/complete-review.md
                cat "$report" >> /tmp/final-report/complete-review.md
                echo "" >> /tmp/final-report/complete-review.md
              fi
            done
          fi
          
          cat /tmp/final-report/complete-review.md

      - name: Create or Update Review Issue
        uses: actions/github-script@main
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            let report = '';
            
            try {
              report = fs.readFileSync('/tmp/final-report/complete-review.md', 'utf8');
            } catch (error) {
              report = '## Review Report\n\nError consolidating reports. Please check workflow logs.';
            }
            
            const date = new Date().toISOString().split('T')[0];
            const title = `Complete CI/CD Review - ${date}`;
            
            const body = `${report}
            
            ## Next Steps - Amazon Q Review
            
            After reviewing these GitHub Copilot agent findings, Amazon Q will provide additional insights:
            - Security analysis
            - Performance optimization opportunities
            - AWS best practices
            - Enterprise architecture patterns
            
            ## Action Items Summary
            
            - [ ] Review and address code cleanliness issues
            - [ ] Fix or improve test coverage
            - [ ] Update documentation as needed
            - [ ] Resolve build issues
            - [ ] Wait for Amazon Q review for additional insights
            
            ---
            *This issue was automatically generated by the Complete CI/CD Review workflow.*
            *Amazon Q review will follow automatically.*
            `;
            
            // Check for existing review issues
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: ['ci-cd-review', 'automated'],
              per_page: 10
            });
            
            const recentIssue = issues.data.find(issue => {
              const createdAt = new Date(issue.created_at);
              const hoursSinceCreation = (Date.now() - createdAt) / (1000 * 60 * 60);
              return hoursSinceCreation < 24;
            });
            
            if (recentIssue) {
              console.log(`Recent issue found: #${recentIssue.number}, updating`);
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: recentIssue.number,
                body: `## Updated Review (${date})\n\n${report}`
              });
            } else {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['ci-cd-review', 'automated', 'needs-review']
              });
            }

      - name: Upload Final Report
        uses: actions/upload-artifact@main
        with:
          name: complete-review-report
          path: /tmp/final-report/complete-review.md
          retention-days: 90

  # Step 6: Trigger Amazon Q Review
  trigger-amazonq:
    runs-on: ubuntu-latest
    needs: consolidate-results
    if: always()
    steps:
      - name: Trigger Amazon Q Review Workflow
        uses: actions/github-script@main
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            console.log('Triggering Amazon Q review workflow...');
            
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: 'auto-amazonq-review.yml',
                ref: context.ref
              });
              console.log('âœ… Amazon Q review workflow triggered successfully');
            } catch (error) {
              console.log(`âš ï¸ Could not trigger Amazon Q review: ${error.message}`);
              console.log('Amazon Q workflow may not be installed yet');
            }
