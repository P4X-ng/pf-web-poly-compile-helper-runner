#!/usr/bin/env python3
"""
Comprehensive Test Runner - "Test it all again and again and again. That's thrice."

This script runs all available tests in the repository three times with fresh 
environment setup between each run, providing detailed reporting and analysis.
"""

import os
import sys
import subprocess
import glob
import json
import time
import shutil
import tempfile
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Tuple, Optional

class TestResult:
    """Container for individual test results"""
    def __init__(self, name: str, success: bool, duration: float, 
                 stdout: str = "", stderr: str = "", returncode: int = 0):
        self.name = name
        self.success = success
        self.duration = duration
        self.stdout = stdout
        self.stderr = stderr
        self.returncode = returncode
        self.timestamp = datetime.now()

class TestRun:
    """Container for a complete test run"""
    def __init__(self, run_number: int):
        self.run_number = run_number
        self.results: List[TestResult] = []
        self.start_time = None
        self.end_time = None
        self.total_duration = 0.0
        
    def add_result(self, result: TestResult):
        self.results.append(result)
        
    def get_success_count(self) -> int:
        return sum(1 for r in self.results if r.success)
        
    def get_failure_count(self) -> int:
        return sum(1 for r in self.results if not r.success)
        
    def get_total_count(self) -> int:
        return len(self.results)

class ComprehensiveTestRunner:
    """Main test runner class"""
    
    def __init__(self, workspace_dir: str = "/workspace"):
        self.workspace_dir = workspace_dir
        self.test_runs: List[TestRun] = []
        self.discovered_tests: List[str] = []
        self.temp_dirs: List[str] = []
        
    def discover_tests(self) -> List[str]:
        """Discover all test files in the repository"""
        print("ğŸ” Discovering test files...")
        
        # Find all test_*.py files
        test_files = []
        
        # Look for test_*.py files
        for pattern in ["test_*.py", "*test*.py"]:
            matches = glob.glob(os.path.join(self.workspace_dir, pattern))
            test_files.extend(matches)
        
        # Also include specific known test files
        known_tests = [
            "quick_test.py",
            "run_syntax_check.py", 
            "simple_syntax_validator.py",
            "test_runner_verification.py"  # Include our verification test
        ]
        
        for test_file in known_tests:
            full_path = os.path.join(self.workspace_dir, test_file)
            if os.path.exists(full_path) and full_path not in test_files:
                test_files.append(full_path)
        
        # Filter out this script itself
        current_script = os.path.abspath(__file__)
        test_files = [f for f in test_files if os.path.abspath(f) != current_script]
        
        # Sort for consistent ordering
        test_files.sort()
        
        self.discovered_tests = test_files
        print(f"ğŸ“Š Discovered {len(test_files)} test files:")
        for test_file in test_files:
            rel_path = os.path.relpath(test_file, self.workspace_dir)
            print(f"  â€¢ {rel_path}")
        
        return test_files
    
    def setup_fresh_environment(self, run_number: int):
        """Set up a fresh environment for testing"""
        print(f"\nğŸ§¹ Setting up fresh environment for run {run_number}...")
        
        # Change to workspace directory
        os.chdir(self.workspace_dir)
        
        # Clean up any previous temporary directories
        for temp_dir in self.temp_dirs:
            if os.path.exists(temp_dir):
                try:
                    shutil.rmtree(temp_dir)
                except Exception as e:
                    print(f"âš ï¸  Warning: Could not clean up {temp_dir}: {e}")
        self.temp_dirs.clear()
        
        # Create a fresh temporary directory for this run
        temp_dir = tempfile.mkdtemp(prefix=f"pf_test_run_{run_number}_")
        self.temp_dirs.append(temp_dir)
        
        # Set environment variables for clean state
        env_vars = {
            'TMPDIR': temp_dir,
            'TEMP': temp_dir,
            'TMP': temp_dir,
            'PF_TEST_RUN': str(run_number),
            'PF_FRESH_ENV': '1'
        }
        
        for key, value in env_vars.items():
            os.environ[key] = value
        
        print(f"âœ… Fresh environment ready (temp dir: {temp_dir})")
        
    def run_single_test(self, test_file: str, timeout: int = 60) -> TestResult:
        """Run a single test file and return results"""
        test_name = os.path.relpath(test_file, self.workspace_dir)
        print(f"  ğŸ§ª Running {test_name}...")
        
        start_time = time.time()
        
        try:
            # Determine how to run the test
            if test_file.endswith('.py'):
                cmd = [sys.executable, test_file]
            elif test_file.endswith('.sh'):
                cmd = ['bash', test_file]
            else:
                # Try to execute directly
                cmd = [test_file]
            
            # Run the test
            result = subprocess.run(
                cmd,
                cwd=self.workspace_dir,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            duration = time.time() - start_time
            success = result.returncode == 0
            
            status = "âœ… PASS" if success else "âŒ FAIL"
            print(f"    {status} ({duration:.2f}s)")
            
            return TestResult(
                name=test_name,
                success=success,
                duration=duration,
                stdout=result.stdout,
                stderr=result.stderr,
                returncode=result.returncode
            )
            
        except subprocess.TimeoutExpired:
            duration = time.time() - start_time
            print(f"    â° TIMEOUT ({duration:.2f}s)")
            return TestResult(
                name=test_name,
                success=False,
                duration=duration,
                stdout="",
                stderr=f"Test timed out after {timeout} seconds",
                returncode=-1
            )
            
        except Exception as e:
            duration = time.time() - start_time
            print(f"    ğŸ’¥ ERROR ({duration:.2f}s): {e}")
            return TestResult(
                name=test_name,
                success=False,
                duration=duration,
                stdout="",
                stderr=str(e),
                returncode=-2
            )
    
    def run_test_suite(self, run_number: int) -> TestRun:
        """Run the complete test suite once"""
        print(f"\nğŸš€ Starting Test Run #{run_number}")
        print("=" * 60)
        
        # Set up fresh environment
        self.setup_fresh_environment(run_number)
        
        # Create test run container
        test_run = TestRun(run_number)
        test_run.start_time = datetime.now()
        
        # Run each test
        for test_file in self.discovered_tests:
            result = self.run_single_test(test_file)
            test_run.add_result(result)
        
        # Finalize run
        test_run.end_time = datetime.now()
        test_run.total_duration = (test_run.end_time - test_run.start_time).total_seconds()
        
        # Print run summary
        print(f"\nğŸ“Š Run #{run_number} Summary:")
        print(f"  â€¢ Total tests: {test_run.get_total_count()}")
        print(f"  â€¢ Passed: {test_run.get_success_count()}")
        print(f"  â€¢ Failed: {test_run.get_failure_count()}")
        print(f"  â€¢ Duration: {test_run.total_duration:.2f}s")
        
        return test_run
    
    def run_all_tests_thrice(self):
        """Run all tests three times as requested"""
        print("ğŸ¯ COMPREHENSIVE TEST RUNNER")
        print("Testing it all again and again and again. That's thrice!")
        print("=" * 70)
        
        # Discover tests
        if not self.discover_tests():
            print("âŒ No tests discovered. Exiting.")
            return False
        
        # Run tests three times
        for run_num in range(1, 4):  # 1, 2, 3
            test_run = self.run_test_suite(run_num)
            self.test_runs.append(test_run)
            
            # Brief pause between runs
            if run_num < 3:
                print(f"\nâ¸ï¸  Brief pause before run #{run_num + 1}...")
                time.sleep(2)
        
        # Generate comprehensive report
        return self.generate_comprehensive_report()
    
    def generate_comprehensive_report(self) -> bool:
        """Generate detailed report across all three runs"""
        print("\n" + "=" * 70)
        print("ğŸ“‹ COMPREHENSIVE TEST REPORT - THREE RUNS ANALYSIS")
        print("=" * 70)
        
        # Overall statistics
        total_tests = len(self.discovered_tests)
        total_executions = total_tests * 3
        
        all_passed = 0
        all_failed = 0
        total_duration = 0.0
        
        for run in self.test_runs:
            all_passed += run.get_success_count()
            all_failed += run.get_failure_count()
            total_duration += run.total_duration
        
        print(f"ğŸ“Š OVERALL STATISTICS:")
        print(f"  â€¢ Total test files: {total_tests}")
        print(f"  â€¢ Total executions: {total_executions} (3 runs Ã— {total_tests} tests)")
        print(f"  â€¢ Total passed: {all_passed}")
        print(f"  â€¢ Total failed: {all_failed}")
        print(f"  â€¢ Success rate: {(all_passed/total_executions)*100:.1f}%")
        print(f"  â€¢ Total duration: {total_duration:.2f}s")
        print(f"  â€¢ Average per run: {total_duration/3:.2f}s")
        
        # Per-run breakdown
        print(f"\nğŸ“ˆ PER-RUN BREAKDOWN:")
        for i, run in enumerate(self.test_runs, 1):
            success_rate = (run.get_success_count() / run.get_total_count()) * 100
            print(f"  Run #{i}: {run.get_success_count()}/{run.get_total_count()} passed "
                  f"({success_rate:.1f}%) in {run.total_duration:.2f}s")
        
        # Test consistency analysis
        print(f"\nğŸ” TEST CONSISTENCY ANALYSIS:")
        test_consistency = {}
        
        for test_file in self.discovered_tests:
            test_name = os.path.relpath(test_file, self.workspace_dir)
            results = []
            for run in self.test_runs:
                for result in run.results:
                    if result.name == test_name:
                        results.append(result.success)
                        break
            
            test_consistency[test_name] = results
        
        # Categorize tests by consistency
        always_pass = []
        always_fail = []
        inconsistent = []
        
        for test_name, results in test_consistency.items():
            if all(results):
                always_pass.append(test_name)
            elif not any(results):
                always_fail.append(test_name)
            else:
                inconsistent.append((test_name, results))
        
        print(f"  â€¢ Always pass: {len(always_pass)} tests")
        print(f"  â€¢ Always fail: {len(always_fail)} tests")
        print(f"  â€¢ Inconsistent: {len(inconsistent)} tests")
        
        # Show details for problematic tests
        if always_fail:
            print(f"\nâŒ ALWAYS FAILING TESTS:")
            for test_name in always_fail:
                print(f"  â€¢ {test_name}")
                # Show error from last run
                for result in self.test_runs[-1].results:
                    if result.name == test_name:
                        if result.stderr:
                            print(f"    Error: {result.stderr[:100]}...")
                        break
        
        if inconsistent:
            print(f"\nâš ï¸  INCONSISTENT TESTS:")
            for test_name, results in inconsistent:
                result_str = "".join("âœ…" if r else "âŒ" for r in results)
                print(f"  â€¢ {test_name}: {result_str}")
        
        # Success determination
        overall_success = len(always_fail) == 0 and len(inconsistent) == 0
        
        print(f"\nğŸ¯ FINAL VERDICT:")
        if overall_success:
            print("ğŸ‰ ALL TESTS CONSISTENTLY PASS ACROSS ALL THREE RUNS!")
            print("âœ… The system is stable and reliable.")
        else:
            print("âš ï¸  Some tests have issues:")
            if always_fail:
                print(f"   - {len(always_fail)} tests consistently fail")
            if inconsistent:
                print(f"   - {len(inconsistent)} tests have inconsistent results")
        
        # Recommendations
        print(f"\nğŸ’¡ RECOMMENDATIONS:")
        if overall_success:
            print("  â€¢ System is ready for production use")
            print("  â€¢ All tests pass consistently")
            print("  â€¢ No action required")
        else:
            print("  â€¢ Investigate failing tests")
            print("  â€¢ Fix inconsistent test behavior")
            print("  â€¢ Consider adding more robust error handling")
        
        print(f"\nğŸ Testing complete. Nay ye canne deny it workes!")
        print("=" * 70)
        
        return overall_success
    
    def cleanup(self):
        """Clean up temporary resources"""
        for temp_dir in self.temp_dirs:
            if os.path.exists(temp_dir):
                try:
                    shutil.rmtree(temp_dir)
                except Exception as e:
                    print(f"âš ï¸  Warning: Could not clean up {temp_dir}: {e}")

def main():
    """Main entry point"""
    # Check for quick verification mode
    quick_mode = len(sys.argv) > 1 and sys.argv[1] == "--quick"
    
    runner = ComprehensiveTestRunner()
    
    try:
        if quick_mode:
            print("ğŸš€ QUICK VERIFICATION MODE")
            print("Running test discovery and single test execution...")
            print("=" * 50)
            
            # Just discover tests and run one quick test
            tests = runner.discover_tests()
            if tests:
                print(f"\nâœ… Successfully discovered {len(tests)} tests")
                print("ğŸ¯ Quick verification complete!")
                return 0
            else:
                print("âŒ No tests discovered")
                return 1
        else:
            success = runner.run_all_tests_thrice()
            return 0 if success else 1
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Test run interrupted by user")
        return 130
    except Exception as e:
        print(f"\n\nğŸ’¥ Unexpected error: {e}")
        return 1
    finally:
        runner.cleanup()

if __name__ == "__main__":
    sys.exit(main())